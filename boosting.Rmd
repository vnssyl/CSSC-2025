---
title: "Data Cleaning"
date: "2025-04-26"
output: pdf_document
---

```{r setup, warning=FALSE, message=FALSE}
# -------------------------------
# INSTALL & LOAD PACKAGES
# -------------------------------
library(data.table)
library(readxl)
library(fst)
library(corrplot)
library(survival)
library(caret)
library(gbm)
library(survivalROC)
library(purrr)
library(survAUC) #For calculating C index
library(mice) # for imp
library(glmnet)
```

```{r data-cleaning, warning=FALSE, message=FALSE}
path <- "C:/Users/johny/OneDrive/Desktop/competition/"
filename <- "synthetic_data_stats_competition_2025_final.xlsx"


# -------------------------------
# STEP 1â€“4: LOAD AND SELECT DATA
# -------------------------------
data <- readxl::read_excel(paste0(path,filename, sep = ""))
data <- as.data.table(data)

# Save & reload as fst for speed
write_fst(data, paste0(path,"data.fst", sep = ""))
data <- read_fst(paste0(path,"data.fst", sep = ""), as.data.table = TRUE)

# ------------------------
# STEP 2: Calculate missing %
# ------------------------
missing_percent <- data[, lapply(.SD, function(x) sum(is.na(x)) / .N)]

# ------------------------
# STEP 3: Define outcome & important labs
# ------------------------
outcome_vars <- c(
  "outcome_afib_aflutter_new_post",
  "time_to_outcome_afib_aflutter_new_post",
  "outcome_all_cause_death",
  "time_to_outcome_all_cause_death",
  "follow_up_duration"
)

drop_vars <- names(missing_percent)[
  (unlist(missing_percent) >= 0.3) & 
  !(names(missing_percent) %in% outcome_vars)
]

cat("Variables dropped:", drop_vars, "\n")

# ------------------------
# STEP 4: Keep final vars
# ------------------------
final_vars <- union(
  union("patient_id", outcome_vars),
  setdiff(names(data), drop_vars)
)

data <- data[, ..final_vars]

# ------------------------
# STEP 5: Correct variable types
# ------------------------
for (col in names(data)) {
  if (is.character(data[[col]])) next
  if (is.numeric(data[[col]])) {
    uniq_vals <- unique(na.omit(data[[col]]))
    if (length(uniq_vals) == 2) {
      data[, (col) := factor(data[[col]])]
    } else {
      data[, (col) := as.numeric(data[[col]])]
    }
  }
}

# ------------------------
# STEP 6: Define survival time and event
# ------------------------
data[, event_afib := as.integer(outcome_afib_aflutter_new_post == 1)]
data[, time_afib := fifelse(
  outcome_afib_aflutter_new_post == 1,
  time_to_outcome_afib_aflutter_new_post,
  fifelse(
    outcome_all_cause_death == 1,
    time_to_outcome_all_cause_death,
    follow_up_duration
  )
)]
data <- data[!is.na(time_afib)]

# ------------------------
# STEP 7: Train-Test Split (30% training, 70% testing)
# ------------------------
set.seed(123)
train_indices <- sample(seq_len(nrow(data)), size = 0.3 * nrow(data))
train_data <- data[train_indices] # This one to feed into model
test_data <- data[-train_indices] # Use this one for AUC, C-index, etc



# ------------------------
# STEP 8: Impute missing values w PMM on train data ONLY
# ------------------------
set.seed(123)

imputed_data <- mice(train_data, method = 'pmm', m = 5, maxit = 10)

# retrieving first completed dataset, set as data.table
data <- complete(imputed_data, 1)
setDT(train_data)


# ------------------------
# STEP 9: Save survival-ready data
# ------------------------
write_fst(data, paste0(path, "data_survival_ready_final.fst", sep = ""))
```

```{r}
# ------------------------
# STEP 10: Variable Selection with Cox Lasso (glmnet)
# ------------------------

library(glmnet)
library(survival)
library(data.table)

data$time_afib[data$time_afib <= 0] <- 1e-8

outcome_vars <- c(
  "patient_id", "event_afib", "time_afib",
  "outcome_afib_aflutter_new_post",
  "time_to_outcome_afib_aflutter_new_post",
  "outcome_all_cause_death",
  "time_to_outcome_all_cause_death",
  "follow_up_duration"
)

# Define predictor variables
predictor_vars <- setdiff(names(data), outcome_vars)

# Keep only numeric predictors
numeric_check <- sapply(data[, predictor_vars], is.numeric)
numeric_vars <- predictor_vars[numeric_check]

# Create matrix for glmnet
x <- as.matrix(data[, numeric_vars])
y <- Surv(time = data$time_afib, event = data$event_afib)

# Initialize storage for all selected variables
selected_all <- c()

# Run Lasso Cox 5 times
set.seed(123)
for (i in 1:5) {
  x <- as.matrix(data[, numeric_vars])
  y <- Surv(time = data$time_afib, event = data$event_afib)
  
  lasso_fit <- cv.glmnet(x, y, family = "cox", alpha = 1)
  coef_matrix <- coef(lasso_fit, s = lasso_fit$lambda.min)
  selected_vars <- rownames(coef_matrix)[which(coef_matrix != 0)]
  
  cat(paste("Run", i, "- Selected variables:\n"))
  print(selected_vars)
  
  selected_all <- c(selected_all, selected_vars)
}

# Create data frame of frequencies
selected_df <- as.data.frame(table(selected_all))
colnames(selected_df) <- c("Variable", "Frequency")
selected_df <- selected_df[order(-selected_df$Frequency), ]

# Show data frame
print(selected_df)

# Plot histogram
barplot(
  selected_df$Frequency,
  names.arg = selected_df$Variable,
  las = 2,
  col = "skyblue",
  main = "Variable Selection Frequency (5 Lasso Runs)",
  ylab = "Selection Count",
  cex.names = 0.7
)

```


```{r}
# Tuning
param_grid <- expand.grid(
  n.trees = c(1000, 2000, 3000),
  interaction.depth = c(1, 3, 5),
  shrinkage = c(0.01, 0.05),
  bag.fraction = c(0.6, 0.8)
)

set.seed(123)
folds <- sample(1:5, nrow(train_data), replace = TRUE)


# Function to evaluate one combo
evaluate_combo <- function(params) {
  cindexes <- numeric(5)

  for (k in 1:5) {
    tune_train_idx <- which(folds != k)
    tune_test_idx <- which(folds == k)

    tune_train_data <- train_data[train_tune_train_idxidx, ]
    tune_test_data <- train_data[tune_test_idx, ]

    model <- gbm(
      formula = Surv(time_afib, event_afib) ~ .,
      data = tune_train_data,
      distribution = "coxph",
      n.trees = params$n.trees,
      interaction.depth = params$interaction.depth,
      shrinkage = params$shrinkage,
      bag.fraction = params$bag.fraction,
      train.fraction = 1.0,  # use full training fold
      verbose = FALSE
    )

    pred <- predict(model, newdata = tune_test_data, n.trees = params$n.trees, type = "link")

    concord <- concordance(Surv(time_afib, event_afib) ~ pred, data = tune_test_data)
    cindexes[k] <- concord$concordance
  }

  mean(cindexes)
}

# Run the tuning
param_grid$cindex <- pmap_dbl(param_grid, evaluate_combo)

best_params <- param_grid %>%
  arrange(desc(cindex)) %>%
  slice(1)

print(best_params)
```

```{r}
# ------------------------
# STEP 10: Survival Boosting with GBM
# ------------------------
predictor_vars <- setdiff(
  names(data),
  c("patient_id", "event_afib", "time_afib", outcome_vars)
)

# Prepare data for training GBM
boost_data <- as.data.frame(train_data[, c("time_afib", "event_afib", predictor_vars), with = FALSE])

# Fit GBM Cox model
set.seed(123)
gbm_fit <- gbm(
  formula = Surv(time_afib, event_afib) ~ demographics_age_index_ecg + creatinine_peri + ecg_resting_qtc + hgb_peri + rdw_peri + chloride_peri + ecg_resting_hr + ecg_resting_pr + plt_peri + potassium_peri + sodium_peri + wbc_peri,
  data = boost_data,
  distribution = "coxph",
  n.trees = 3000,
  interaction.depth = 3,
  shrinkage = 0.01,
  bag.fraction = 0.7,
  train.fraction = 0.8,
  cv.folds = 5,
  n.minobsinnode = 10,
  verbose = FALSE
)

# Select best iteration using cross-validation
best_iter <- gbm.perf(gbm_fit, method = "cv")

# Variable importance plot
importance <- summary(gbm_fit, n.trees = best_iter, plotit = FALSE)
barplot(
  importance[1:20, "rel.inf"], 
  names.arg = importance[1:20, "var"],
  las = 2,
  main = "Top 20 Variables (GBM Boosting)"
)
```

```{r}
# ------------------------
# STEP 12: Variable Importance Table(Base R + data.table only)
# ------------------------

# Get importance from GBM
importance <- summary(gbm_fit, n.trees = best_iter, plotit = FALSE)

# Convert to data.table
importance_dt <- as.data.table(importance)
setnames(importance_dt, c("var", "rel.inf"), c("Variable", "RelativeInfluence"))

# Round values for readability
importance_dt[, RelativeInfluence := round(RelativeInfluence, 2)]

# Sort by importance descending
importance_dt <- importance_dt[order(-RelativeInfluence)]

# Print top 20 variables as a normal table
print(importance_dt[1:20])

```


```{r}
# Predict risk scores (linear predictor) on test set
test_boost_data <- as.data.frame(test_data[, predictor_vars, with = FALSE])
predicted_risk <- predict(gbm_fit, newdata = test_boost_data, n.trees = best_iter, type = "link")
test_data[, predicted_risk := predicted_risk]
```


#Performance Scores
```{r}
#4 year
eval_time <- 1480

T_train <- train_data$time_afib
delta_train <- train_data$event_afib

T_test <- test_data$time_afib
delta_test <- test_data$event_afib
lp_test <- test_data$predicted_risk

auc_uno <- AUC.uno(
  Surv.rsp = Surv(T_train, delta_train),    # training survival object
  Surv.rsp.new = Surv(T_test, delta_test),  # testing survival object
  lpnew = lp_test,                          # predicted risk (linear predictor)
  times = eval_time                         # time at which to evaluate
)
str(auc_uno)

#5 year
eval_time <- 1825
T_train <- train_data$time_afib
delta_train <- train_data$event_afib

T_test <- test_data$time_afib
delta_test <- test_data$event_afib
lp_test <- test_data$predicted_risk

auc_uno <- AUC.uno(
  Surv.rsp = Surv(T_train, delta_train),    # training survival object
  Surv.rsp.new = Surv(T_test, delta_test),  # testing survival object
  lpnew = lp_test,                          # predicted risk (linear predictor)
  times = eval_time                         # time at which to evaluate
)
str(auc_uno)

#6 months
eval_time <- 182
T_train <- train_data$time_afib
delta_train <- train_data$event_afib

T_test <- test_data$time_afib
delta_test <- test_data$event_afib
lp_test <- test_data$predicted_risk

auc_uno <- AUC.uno(
  Surv.rsp = Surv(T_train, delta_train),
  Surv.rsp.new = Surv(T_test, delta_test),
  lpnew = lp_test,
  times = eval_time
)
str(auc_uno)
```
# Formatted nicer
```{r}
T_train <- train_data$time_afib
delta_train <- train_data$event_afib
T_test <- test_data$time_afib
delta_test <- test_data$event_afib
lp_test <- test_data$predicted_risk

# 6-month AUC
auc_6mo <- AUC.uno(
  Surv.rsp = Surv(T_train, delta_train),
  Surv.rsp.new = Surv(T_test, delta_test),
  lpnew = lp_test,
  times = 182
)

# 4-year AUC
auc_4yr <- AUC.uno(
  Surv.rsp = Surv(T_train, delta_train),
  Surv.rsp.new = Surv(T_test, delta_test),
  lpnew = lp_test,
  times = 1480
)

# 5-year AUC
auc_5yr <- AUC.uno(
  Surv.rsp = Surv(T_train, delta_train),
  Surv.rsp.new = Surv(T_test, delta_test),
  lpnew = lp_test,
  times = 1825
)

# Print AUC values
cat("6-month AUC:", auc_6mo$auc, "\n")
cat("4-year AUC:", auc_4yr$auc, "\n")
cat("5-year AUC:", auc_5yr$auc, "\n")
```
